{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## CS 634 Data Mining Midterm Term Project -- Tap47"
      ],
      "metadata": {
        "id": "-_k7A75za50f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github: https://github.com/tap4725/Midterm_term_project"
      ],
      "metadata": {
        "id": "Kf_rEIoSj6TB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction"
      ],
      "metadata": {
        "id": "VpQsa0jcW_L-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook provides a full overview of the Apriori algorithm implementation and application in association rule mining. The source.py file provides a custom implementation of the Apriori algorithm, and the Jupyter notebook shows how to apply this implementation to a dataset."
      ],
      "metadata": {
        "id": "LZiNvJ-SXEJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Abstract"
      ],
      "metadata": {
        "id": "ZOxta1ESgKkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This report presents a comprehensive analysis of the Apriori algorithm, a foundational method for association rule mining used to identify relationships among items in transactional data. The primary objective was to implement the Apriori algorithm from scratch and apply it to a dataset. The analysis involved the development of a custom apriori class that calculates support, generates frequent itemsets, and derives association rules."
      ],
      "metadata": {
        "id": "rqmXw8WlgD-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Apriori Algorithm Implementation (source.py)"
      ],
      "metadata": {
        "id": "XErTpKemXN8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The apriori class is defined in the source.py file, and it has methods for estimating support, creating frequent itemsets, and finding association rules. The class and its functions are described in detail below.\n"
      ],
      "metadata": {
        "id": "nn1bT3m9XPla"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1\n",
        "```\n",
        "__init__(self, series)\n",
        "  Purpose: Initializes the class with a series of transactions.\n",
        "  Parameter: series â€“ A pandas Series containing transaction data (each entry is a list of items).\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1qF5FW1ZXXvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 calculate_support(self, items)\n"
      ],
      "metadata": {
        "id": "_6HKPOZeXjtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function computes the support of an itemset, which is the percentage of transactions containing all the items in that set."
      ],
      "metadata": {
        "id": "wi-BkB0MXmxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "def calculate_support(self, items):\n",
        "    transactions = self.series\n",
        "    counts = np.array([0] * len(self.series))\n",
        "    for item in items:\n",
        "        counts += transactions.str.count(item)\n",
        "    return (np.where(counts >= len(items), 1, 0).sum() / len(transactions)) * 100\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_joNhWn8Xtkb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3 get_unique_items(self, transactions=None)"
      ],
      "metadata": {
        "id": "ogapJ9rBXw9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method extracts all unique items from the transactions.\n",
        "\n",
        "\n",
        "```\n",
        "def get_unique_items(self, transactions=None):\n",
        "    if not transactions:\n",
        "        transactions = self.series\n",
        "    items = np.array([])\n",
        "    for transaction in transactions:\n",
        "        items = np.append(items, transaction.replace(\" \", \"\").split(\",\"))\n",
        "    return set(items)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "mfUeXzqqXyyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4 generate_item_sets(self, items, k=0)"
      ],
      "metadata": {
        "id": "Np-3hnqeX6Gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generates combinations of items of size k.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "def generate_item_sets(self, items, k=0):\n",
        "    return list(itertools.combinations(items, r=k))\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ai-NWaVGX9SK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.5 get_freq_items(self, min_support, items=None)"
      ],
      "metadata": {
        "id": "rUremiH2YAjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finds all frequent itemsets that have support greater than or equal to min_support.\n",
        "\n",
        "\n",
        "```\n",
        "def get_freq_items(self, min_support, items=None):\n",
        "    if not items:\n",
        "        items = self.get_unique_items()\n",
        "    count = 1\n",
        "    freq_items = []\n",
        "    while True:\n",
        "        temp = []\n",
        "        for i in self.generate_item_sets(items, count):\n",
        "            support = self.calculate_support(i)\n",
        "            if support >= min_support:\n",
        "                print(f\"Support for item-set {i}: {support}\")\n",
        "                temp.append(i)\n",
        "        if len(temp) > 0:\n",
        "            freq_items += temp\n",
        "            count += 1\n",
        "        else:\n",
        "            break\n",
        "    return freq_items\n",
        "```\n"
      ],
      "metadata": {
        "id": "uQwJ89-cYB0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.6 get_associations(self, freq_items, min_conf)"
      ],
      "metadata": {
        "id": "WyI9kDjGYJ4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generates association rules based on the frequent itemsets and a minimum confidence threshold.\n",
        "\n",
        "\n",
        "```\n",
        "def get_associations(self, freq_items, min_conf):\n",
        "    temp = []\n",
        "    for _ in freq_items:\n",
        "        if len(_) == 1:\n",
        "            continue\n",
        "        for items in itertools.permutations(_):\n",
        "            try:\n",
        "                conf = (self.calculate_support(items) / self.calculate_support(items[:-1])) * 100\n",
        "            except:\n",
        "                continue\n",
        "            if conf >= min_conf and conf <= 100:\n",
        "                temp.append([items, conf])\n",
        "    print(\"Associations:\", temp)\n",
        "    return temp\n",
        "```"
      ],
      "metadata": {
        "id": "Rip5FR_AYK3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Applying and comparing the Apriori Algorithm to existing algorithm in the Jupyter Notebook"
      ],
      "metadata": {
        "id": "WPm4VCe0YQhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The notebook (tap47.ipynb) explains how the Apriori algorithm is used on a dataset. This is how the process works:\n",
        "\n",
        "4.1 Dataset loading\n",
        "The dataset for analysis is imported from a CSV file, and transactions are renamed for clarity.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "from source import apriori\n",
        "df = pd.read_csv(\"data/assignment1.csv\")\n",
        "df = df.rename(columns={\"Transaction\": \"Items\"})\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "l5qTjiBOYdTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.2 Preprocessing with One-Hot Encoding"
      ],
      "metadata": {
        "id": "_gnvLPbZYkG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding is implemented using the MultiLabelBinarizer from sklearn.preprocessing. Each transaction is divided into separate elements, and a binary representation is generated.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Preprocessing: splitting items and applying one-hot encoding\n",
        "test = [i.replace(\" \", \"\").split(\",\") for i in np.array(df[\"Items\"])]\n",
        "mlb = MultiLabelBinarizer()\n",
        "res = pd.DataFrame(mlb.fit_transform(test),\n",
        "                   columns=mlb.classes_)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "MoOvsfEXY9IN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.3 Compared my algorithm using external packages (efficient-apriori, mlxtend)."
      ],
      "metadata": {
        "id": "ZQ01MuDtZaVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.3.1 Efficient-algorithm"
      ],
      "metadata": {
        "id": "OasbI14cZ8Dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from efficient_apriori import apriori as appp\n",
        "itemsets, rules = appp([i.replace(\" \", \"\").split(\",\") for i in np.array(df[\"Items\"])] , min_support=0.5, min_confidence=0.5)\n",
        "rules\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "WDCc3Sg9Z_JU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.3.2 mlxtend apriori"
      ],
      "metadata": {
        "id": "x2jUbxEDaBNO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "from mlxtend.frequent_patterns import apriori as fpgrowth, association_rules, apriori as appriori\n",
        "frequent_itemsets = appriori(res, min_support=0.5, use_colnames= True)\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
        "rules\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Lw05WkpCZ3ia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.3.3 mlxtend fp growth tree"
      ],
      "metadata": {
        "id": "C43h4EOuaDZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "frequent_itemsets = fpgrowth(res, min_support=0.5, use_colnames= True)\n",
        "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
        "rules\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "XVxxQO0DaMqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Conclusion"
      ],
      "metadata": {
        "id": "EvyaSDCHasgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this study, I successfully developed the Apriori algorithm for association rule mining, demonstrating its usefulness in identifying associations in transactional data. The bespoke implementation, wrapped in the apriori class in source.py, offered a thorough grasp of the algorithm's mechanics, including support and confidence calculations."
      ],
      "metadata": {
        "id": "yvROKy8sawOr"
      }
    }
  ]
}